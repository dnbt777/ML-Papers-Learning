For the first part of the transformer


x = list of embeddings (vectors) of the input tokens // (i.e. "a b b" => [[0.2,...], [0.82,...],[0.82,...]])

for i in range(N):
    // attention phase
    dx = do self-attention of the list of embeddings
    x += dx
    x = normalize(x) // (i.e. shift+shrink x so the avg is 0 and std 1, helps models work better)

    // feed forward phase
    dx = MLP(x)
    x += dx
    x = normalize(x)

    //keep repeating for.. some arbitrary amnt of reps





What is attention? V Q K
En Em are embeddings for n and m
Q 			is the context surrounding n
K 			is m's role in that context / how it is relevant
Q*K 		is how relevant m is to n (how much m 'attends' to n)
V			the ways that m changes n, m is when relevant to n
(Q*K)*V		the ways that m changes n (dx)











unanswered Qs:
what precisely is multi-head attention and is it different from normal self-attention
what is "masked" multi head attention


most important question: why does this work?




to make:
run 100 transformers. they learn - but what is the learning rate? could be 0.01. could be 0.1. etc. but, what if, instead, it was a matrix of weights, W_L, that gets applied to the loss gradient?
so, run 100 transformers, with randomized weights for W_L. they will calculate their losses, which will be matmul'd with W_L, and then this combination will be used to shift the weights of the transformer.
    (As opposed to, for example, the )
The top 10 transformers will pass on their "genes" of learning.
The transformers will learn how to learn the dataset.